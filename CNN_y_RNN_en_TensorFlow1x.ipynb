{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPNlccBIbr6E4MBmiQslXDl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/juancaalcaraz/practicaML/blob/main/CNN_y_RNN_en_TensorFlow1x.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modelo CNN para clasificación de imagenes."
      ],
      "metadata": {
        "id": "rhY5_Cmg4ndZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "E0Zrppn74VwR"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "from tensorflow import keras\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargar el conjunto de datos MNIST\n",
        "(X_train, y_train), (X_test, y_test) = datasets.mnist.load_data()\n",
        "\n",
        "# Imprimir la forma de los datos de entrenamiento\n",
        "print(X_train.shape)  # Salida: (60000, 28, 28)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gjWkM0eO65fs",
        "outputId": "aaa6390e-753f-4077-a883-1a6e4cb649ed"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "(60000, 28, 28)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "shape = (28, 28, 1)\n",
        "model = models.Sequential([\n",
        "    keras.Input(shape=shape),\n",
        "    layers.Conv2D(32, (5, 5), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(64, (5, 5), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(10, activation='softmax')\n",
        "])\n",
        "# Compilar el modelo\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "TcoMJNIU73K3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "21A4u-8p9-Qv",
        "outputId": "8cf6c7f0-d1f7-4128-f937-37d5e35a8894"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ conv2d_4 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │             \u001b[38;5;34m832\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ max_pooling2d_4 (\u001b[38;5;33mMaxPooling2D\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv2d_5 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m64\u001b[0m)            │          \u001b[38;5;34m51,264\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ max_pooling2d_5 (\u001b[38;5;33mMaxPooling2D\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m64\u001b[0m)            │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ flatten_2 (\u001b[38;5;33mFlatten\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)                │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │         \u001b[38;5;34m131,200\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)                  │           \u001b[38;5;34m1,290\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ conv2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">832</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ max_pooling2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv2d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            │          <span style=\"color: #00af00; text-decoration-color: #00af00\">51,264</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ max_pooling2d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ flatten_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)                │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │         <span style=\"color: #00af00; text-decoration-color: #00af00\">131,200</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,290</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m184,586\u001b[0m (721.04 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">184,586</span> (721.04 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m184,586\u001b[0m (721.04 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">184,586</span> (721.04 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=1, min_lr=0.00001)\n",
        "early_stopping = EarlyStopping(monitor='accuracy', patience=5, restore_best_weights=True, verbose=1)\n",
        "\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=64, callbacks=[reduce_lr, early_stopping])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_AvzkJ9C-L3C",
        "outputId": "ffb365c7-8676-4299-93b2-b489912b233a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 68ms/step - accuracy: 0.7777 - loss: 2.0077 - learning_rate: 0.0010\n",
            "Epoch 2/10\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 64ms/step - accuracy: 0.9641 - loss: 0.1327 - learning_rate: 0.0010\n",
            "Epoch 3/10\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 66ms/step - accuracy: 0.9723 - loss: 0.0950 - learning_rate: 0.0010\n",
            "Epoch 4/10\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 68ms/step - accuracy: 0.9786 - loss: 0.0733 - learning_rate: 0.0010\n",
            "Epoch 5/10\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 65ms/step - accuracy: 0.9796 - loss: 0.0690 - learning_rate: 0.0010\n",
            "Epoch 6/10\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 65ms/step - accuracy: 0.9828 - loss: 0.0644 - learning_rate: 0.0010\n",
            "Epoch 7/10\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 66ms/step - accuracy: 0.9831 - loss: 0.0606 - learning_rate: 0.0010\n",
            "Epoch 8/10\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 64ms/step - accuracy: 0.9850 - loss: 0.0527 - learning_rate: 0.0010\n",
            "Epoch 9/10\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 66ms/step - accuracy: 0.9862 - loss: 0.0476 - learning_rate: 0.0010\n",
            "Epoch 10/10\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 65ms/step - accuracy: 0.9878 - loss: 0.0445 - learning_rate: 0.0010\n",
            "Restoring model weights from the end of the best epoch: 10.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7e01154b2ec0>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluación del modelo\n",
        "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
        "\n",
        "print('Test accuracy:', test_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9_f6KHZLGF9j",
        "outputId": "24cf4333-0593-494b-ae95-5a12281c53c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - accuracy: 0.9836 - loss: 0.0693\n",
            "Test accuracy: 0.9869999885559082\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementar una CNN con la API de bajo nivel de TensorFlow"
      ],
      "metadata": {
        "id": "DOJS94M3kDLH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vamos a crear una función para iterar a travez los minilotes de datos.\n",
        "def batch_generator(X, y, batch_size=64, shuffle=False, random_seed=None):\n",
        "  \"\"\"\n",
        "  Generador de mini lotes de datos.\n",
        "\n",
        "  Esta función devuelve un generador que itera a través de los mini lotes de datos\n",
        "  de entrada `X` y las etiquetas `y`. Es útil para entrenar modelos de Machine Learning\n",
        "  cuando se desea procesar los datos en mini lotes en lugar de todo el conjunto de datos\n",
        "  de una vez.\n",
        "\n",
        "  Parámetros:\n",
        "  X: np.ndarray\n",
        "      Matriz de características de entrada, donde cada fila corresponde a una muestra de datos.\n",
        "  y: np.ndarray\n",
        "      Vector de etiquetas correspondientes a las muestras de `X`.\n",
        "  batch_size: int, opcional (default=64)\n",
        "      El tamaño de cada mini lote. Determina cuántas muestras se devuelven en cada iteración.\n",
        "  shuffle: bool, opcional (default=False)\n",
        "      Si es `True`, las muestras se barajan aleatoriamente antes de la creación de los mini lotes.\n",
        "  random_seed: int, opcional (default=None)\n",
        "      Semilla para el generador de números aleatorios. Se usa solo si `shuffle=True` para asegurar\n",
        "      la reproducibilidad del orden aleatorio.\n",
        "\n",
        "  Yields:\n",
        "  tuple (X_batch, y_batch)\n",
        "      - X_batch: np.ndarray\n",
        "        Un mini lote de datos de entrada, con forma `(batch_size, X.shape[1])`.\n",
        "      - y_batch: np.ndarray\n",
        "        Un mini lote de etiquetas, con forma `(batch_size,)`.\n",
        "\n",
        "  Notas:\n",
        "  - La función no devuelve una lista completa de mini lotes, sino que usa `yield` para\n",
        "    generar los mini lotes uno por uno, lo que la hace adecuada para trabajar con grandes\n",
        "    conjuntos de datos que no caben completamente en memoria.\n",
        "  \"\"\"\n",
        "  idx = np.arange(y.shape[0])\n",
        "  if shuffle:\n",
        "    rng = np.random.RandomState(random_seed)\n",
        "    rng.shuffle(idx)\n",
        "    X = X[idx]\n",
        "    y = y[idx]\n",
        "  for i in range(0, X.shape[0], batch_size):\n",
        "    yield (X[i:i+batch_size, :], y[i:i+batch_size])"
      ],
      "metadata": {
        "id": "0VhBZ5iIkPlW"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Para implementar una CNN en TensorFlow definimos dos funciones de envoltorio que harán la construcción de la red más sencilla. Una función de envoltorio para una capa convolucional y otra para la capa completamente conectada."
      ],
      "metadata": {
        "id": "pA2coSPdnAGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def conv_layer(input_tensor, name, kernel_size, n_output_channels, padding_mode='SAME', strides=(1, 1, 1, 1)):\n",
        "  \"\"\"\n",
        "  #################################################################################\n",
        "  # Esta función crea una capa convolucional.                                     #\n",
        "  #################################################################################\n",
        "  parameters:\n",
        "  input_tensor: tf.tensor, tensor de enrtada\n",
        "  name: str Nombre de la capa\n",
        "  kernel_size: int tamaño del kernel\n",
        "  n_output_channels: int Número de canales de salida\n",
        "  padding_mode: str, tipo de padding 'SAME', 'EXPLICIT' o 'VALID' (default='SAME')\n",
        "  strides: tuple (default=(1, 1, 1,))\n",
        "  returns:\n",
        "  tf.tensor: tensor de salida\n",
        "  \"\"\"\n",
        "  with tf.compat.v1.variable_scope(name):\n",
        "    ## obtener n_inputs_channels:\n",
        "    ## Forma del tensor de entrada\n",
        "    ## [batch_size x width x height x channels_in]\n",
        "    input_shape = input_tensor.get_shape().as_list()\n",
        "    n_input_channels = input_shape[-1]\n",
        "\n",
        "    weight_shape = list(kernel_size) + [n_input_channels, n_output_channels]\n",
        "    weights = tf.Variable(tf.random.truncated_normal(weight_shape, stddev=0.01), name='_weights')\n",
        "    print(weights)\n",
        "\n",
        "    biases = tf.Variable(tf.zeros(shape=[n_output_channels]), name='_biases')\n",
        "    print(biases)\n",
        "    conv = tf.nn.conv2d(input=input_tensor, filters=weights, strides=strides, padding=padding_mode)\n",
        "    print(conv)\n",
        "    conv = tf.nn.bias_add(conv, biases, name='net_pre_activation')\n",
        "    print(conv)\n",
        "    conv = tf.nn.relu(conv, name='activation')\n",
        "    print(conv)\n",
        "    return conv"
      ],
      "metadata": {
        "id": "niK_ZR7Bn1fN"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# probamos la funcion con un simple gráfico\n",
        "g = tf.Graph()\n",
        "with tf.compat.v1.Graph().as_default() as g:\n",
        "  x = tf.compat.v1.placeholder(tf.float32, shape=[None, 28, 28, 1])\n",
        "  conv_layer(x, name='conv_test', kernel_size=(3, 3), n_output_channels=20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SzcwcVkQu5va",
        "outputId": "a291c49a-b8c3-46f4-cb4f-648e23fa013c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<tf.Variable 'conv_test/_weights:0' shape=(3, 3, 1, 20) dtype=float32>\n",
            "<tf.Variable 'conv_test/_biases:0' shape=(20,) dtype=float32>\n",
            "Tensor(\"conv_test/Conv2D:0\", shape=(None, 28, 28, 20), dtype=float32)\n",
            "Tensor(\"conv_test/net_pre_activation:0\", shape=(None, 28, 28, 20), dtype=float32)\n",
            "Tensor(\"conv_test/activation:0\", shape=(None, 28, 28, 20), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "del g, x"
      ],
      "metadata": {
        "id": "jPre9gUf4KnJ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mismo codigo anterior pero optimizado para tensorFlow 2.x"
      ],
      "metadata": {
        "id": "pHc1OFPW-x10"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "@tf.function\n",
        "def conv_layer(input_tensor, name, kernel_size, n_output_channels, padding_mode='same', strides=(1, 1, 1, 1)):\n",
        "    \"\"\"\n",
        "    Esta función crea una capa convolucional en TensorFlow 2.x.\n",
        "\n",
        "    Parameters:\n",
        "    input_tensor: tf.Tensor, tensor de entrada.\n",
        "    name: str, nombre de la capa.\n",
        "    kernel_size: int, tamaño del kernel.\n",
        "    n_output_channels: int, número de canales de salida.\n",
        "    padding_mode: str, tipo de padding ('same' o 'valid'). Default es 'same'.\n",
        "    strides: tuple, pasos (default es (1, 1, 1, 1)).\n",
        "\n",
        "    Returns:\n",
        "    tf.Tensor, tensor resultante de la capa convolucional.\n",
        "    \"\"\"\n",
        "    with tf.name_scope(name):\n",
        "        # Obtener número de canales de entrada\n",
        "        input_shape = input_tensor.shape.as_list()\n",
        "        n_input_channels = input_shape[-1]\n",
        "\n",
        "        # Definir la forma de los pesos\n",
        "        weight_shape = list(kernel_size) + [n_input_channels, n_output_channels]\n",
        "\n",
        "        # Inicializar los pesos y sesgos\n",
        "        weights = tf.Variable(tf.random.normal(weight_shape), name='_weights')\n",
        "        biases = tf.Variable(tf.zeros([n_output_channels]), name='_biases')\n",
        "\n",
        "        # Realizar la convolución\n",
        "        conv = tf.nn.conv2d(input_tensor, filters=weights, strides=strides, padding=padding_mode)\n",
        "\n",
        "        # Sumar el sesgo\n",
        "        conv = tf.nn.bias_add(conv, biases, name='net_pre_activation')\n",
        "\n",
        "        # Aplicar ReLU como función de activación\n",
        "        conv = tf.nn.relu(conv, name='activation')\n",
        "\n",
        "        return conv\n"
      ],
      "metadata": {
        "id": "FJeoXFGN-3ci"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## La siguiente función sirve para definir nuestras capas completamente conectadas."
      ],
      "metadata": {
        "id": "13KCgkBm4Pi0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fc_layer(input_tensor, name, n_output_units, activation_fn=None):\n",
        "  \"\"\"\n",
        "  #################################################################################\n",
        "  # Esta función crea una capa completamente conectada.                           #\n",
        "  #################################################################################\n",
        "  \"\"\"\n",
        "  with tf.name_scope(name):\n",
        "    input_shape = input_tensor.get_shape().as_list()[1:]\n",
        "    n_inputs_units = np.prod(input_shape)\n",
        "    if len(input_shape) > 1:\n",
        "      input_tensor = tf.reshape(input_tensor, shape=(-1, n_inputs_units))\n",
        "    weights_shape = [n_inputs_units, n_output_units]\n",
        "    weights = tf.Variable(initial_value=tf.random.normal(weights_shape), name=\"weights\")\n",
        "    print(weights)\n",
        "    biases = tf.Variable(tf.zeros(shape=[n_output_units]), name='biases')\n",
        "    print(biases)\n",
        "    layer = tf.matmul(input_tensor, weights)\n",
        "    print(layer)\n",
        "    layer = tf.nn.bias_add(layer, biases)\n",
        "    print(layer)\n",
        "    if activation_fn is None:\n",
        "      return layer\n",
        "    layer = activation_fn(layer, name='activation')\n",
        "    print(layer)\n",
        "    return layer"
      ],
      "metadata": {
        "id": "5kyoAZ1r4ZLo"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## La función *fc_layers* construye los pesos y los sesgos, los inicializa y luego realiza una multiplicación de matrices mediante la función *tf.matmul*.\n",
        "## La función fc_layers cuenta con tres argumentos obligatorios:\n",
        "1. *input_tensor*: El tensor de entrada.\n",
        "2. *name*: El nombre de la capa, que se utiliza como nombre del alcance.\n",
        "3. *n_output_units*: El número de unidades de salida."
      ],
      "metadata": {
        "id": "bnfxzYeK9Rjw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# probamos la funcion con un simple gráfico\n",
        "g = tf.Graph()\n",
        "with tf.compat.v1.Graph().as_default() as g:\n",
        "  x = tf.compat.v1.placeholder(tf.float32, shape=[None, 28, 28, 1])\n",
        "  fc_layer(x, name='fc_test', n_output_units=32, activation_fn=tf.nn.relu)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lu2_Og5y9weT",
        "outputId": "b0e436e4-a7cb-4546-93df-76adb1ec0713"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<tf.Variable 'fc_test/weights:0' shape=(784, 32) dtype=float32>\n",
            "<tf.Variable 'fc_test/biases:0' shape=(32,) dtype=float32>\n",
            "Tensor(\"fc_test/MatMul:0\", shape=(None, 32), dtype=float32)\n",
            "Tensor(\"fc_test/BiasAdd:0\", shape=(None, 32), dtype=float32)\n",
            "Tensor(\"fc_test/activation:0\", shape=(None, 32), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ahora podemos utilizar estas funciones de envoltorio para construir toda la red convolucional. Definimos una función denominada *buid_cnn* para gestionar la construcción del modelo."
      ],
      "metadata": {
        "id": "FDFnlHT3B2BF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_cnn():\n",
        "  ## Marcadores de posiones para X e y:\n",
        "  tf_x = tf.compat.v1.placeholder(tf.float32, shape=[None, 784], name='tf_x')\n",
        "  tf_y = tf.compat.v1.placeholder(tf.int32, shape=None, name='tf_y')\n",
        "  # Remodelar tf_x en u tensor 4D:\n",
        "  # [batch_size, width, height, 1]\n",
        "  tf_x_image = tf.reshape(tf_x, shape=[-1, 28, 28, 1], name='tf_x_reshaped')\n",
        "  # Codificacion one-hot:\n",
        "  tf_y_onehot = tf.one_hot(indices=tf_y, depth=10, dtype=tf.float32, name='tf_y_onehot')\n",
        "  # 1. capa: Conv_1\n",
        "  print('\\n Construyendo la 1. capa: ')\n",
        "  h1 = conv_layer(tf_x_image, name='conv_1', kernel_size=(5, 5), padding_mode='VALID', n_output_channels=32)\n",
        "  ## Agrupación máxima:\n",
        "  h1_pool = tf.nn.max_pool(h1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
        "  # 2. capa: Conv_2\n",
        "  print('\\n Construyendo la 2. capa: ')\n",
        "  h2  = conv_layer(h1_pool, name='conv_2', kernel_size=(5, 5), padding_mode='VALID', n_output_channels=64)\n",
        "  ## Agrupació máxima:\n",
        "  h2_pool = tf.nn.max_pool(h2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
        "  ## Capa: 3 completamente conectada:\n",
        "  print('\\n Contruyendo la 3. capa: ')\n",
        "  h3 = fc_layer(h2_pool, name='fc_3', n_output_units=1024, activation_fn=tf.nn.relu)\n",
        "  ## Dropout:\n",
        "  keep_prob = tf.compat.v1.placeholder(tf.float32, name='fc_keep_prob')\n",
        "  h3_drop = tf.nn.dropout(h3, rate=1-keep_prob, name='dropout_3')\n",
        "  #h3_drop = tf.nn.dropout(h3, keep_prob=keep_prob, name='dropout_layer')\n",
        "  ## 4. capa: Completamente conectada (Activación líneal)\n",
        "  print('\\n Construyendo la 4. capa: ')\n",
        "  h4 = fc_layer(h3_drop, name='fc_4', n_output_units=10, activation_fn=None)\n",
        "  ## Predicciones:\n",
        "  predictions = {'probabilities': tf.nn.softmax(h4, name='probabilities'),\n",
        "                 'labels': tf.cast(tf.argmax(h4, axis=1), tf.int32, name='labels')}\n",
        "  ## Función de pérdida y optimización\n",
        "  cross_entropy_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=h4, labels= tf_y_onehot), name='cross_entropy_loss')\n",
        "  ## Optimizador:\n",
        "  optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate)\n",
        "  optimizer = optimizer.minimize(cross_entropy_loss, name='train_op')\n",
        "  ## Calcular la precisió de la predicción:\n",
        "  correct_predictions = tf.equal(predictions['labels'], tf_y, name='correct_preds')\n",
        "  accuracy = tf.reduce_mean(tf.cast(correct_predictions, tf.float32), name='accuracy')"
      ],
      "metadata": {
        "id": "xaEHDvW4COaN"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ahora defineremos 4 funciones más: *save* y *load* para guardar y cargar los puntos de control del modelo entrenado, *training_set* y *predict* para obtener probabilidades de predicciones o etiquetas de predicción de los datos de prueba."
      ],
      "metadata": {
        "id": "D6njp5fsVpAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "def save(saver, sess, epoch, path='./model/'):\n",
        "  \"\"\"\n",
        "  Función para guardar los pesos, las variables y el estado\n",
        "  del modelo entrenado en tensorFlow 1.x\n",
        "  \"\"\"\n",
        "  if not os.path.isdir(path):\n",
        "    os.mkdir('path')\n",
        "  print(f'guardando el modelo en {path}')\n",
        "  saver.save(sess, os.path.join(path, 'cnn-model.ckpt'), global_step=epoch)\n",
        "  print('modelo guardado')\n"
      ],
      "metadata": {
        "id": "XliRWaZkWdaN"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load(saver, sess, path, epoch):\n",
        "  \"\"\"\n",
        "  Función para cargar los pesos, las variables y el estado\n",
        "  del modelo entrenado en tensorFlow 1.x\n",
        "  \"\"\"\n",
        "  print(f'cargando el modelo desde {path}')\n",
        "  saver.restore(sess, os.path.join(path, f'cnn-model.ckpt-{epoch}'))"
      ],
      "metadata": {
        "id": "5VXgFkTrXIfU"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(sess, training_set, validation_set=None, initializer= True, epoch=20, shuffle=True, dropout=0.5, random_seed=None):\n",
        "  \"\"\"\n",
        "  Función para entrenar el modelo en tensorFlow 1.x\n",
        "  \"\"\"\n",
        "  X_data = np.array(training_set[0])\n",
        "  y_data = np.array(training_set[1])\n",
        "  training_loss = []\n",
        "  ## Inicializar variables:\n",
        "  if initializer:\n",
        "    sess.run(tf.compat.v1.global_variables_initializer())\n",
        "  np.random.seed(random_seed) #para Shuffle en batch_generator.\n",
        "  for epoch in range(1, epoch + 1):\n",
        "    batch_gen = batch_generator(X_data, y_data, shuffle=shuffle)\n",
        "    avg_loss = 0.0\n",
        "    for i, (batch_x, batch_y) in enumerate(batch_gen):\n",
        "      batch_x = batch_x.reshape(batch_x.shape[0], -1)\n",
        "      feed_dict = {'tf_x: 0': batch_x, 'tf_y: 0': batch_y, 'fc_keep_prob: 0':dropout}\n",
        "      loss, _ = sess.run(['cross_entropy_loss:0', 'train_op'], feed_dict=feed_dict)\n",
        "      avg_loss += loss\n",
        "    training_loss.append(avg_loss/(i+1))\n",
        "    print(f'epoch: {epoch} training_loss: {training_loss[-1]:.4f}')\n",
        "    if validation_set is not None:\n",
        "      feed = {'tf_x:0': validation_set[0], 'tf_y:0': validation_set[1], 'fc_keep_prob:0': 1.0}\n",
        "      valid_acc = sess.run('accuracy:0', feed_dict=feed)\n",
        "      print(f'valid_acc: {valid_acc:.3f}')\n",
        "    else :\n",
        "      print()"
      ],
      "metadata": {
        "id": "REqJy_CVYVc-"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(sess, X_test, return_proba=False):\n",
        "  \"\"\"\n",
        "  Función para realizar predicciones en tensorFlow 1.x\n",
        "  \"\"\"\n",
        "  feed_dict = {'tf_x:0': X_test, 'fc_keep_prob:0': 1.0}\n",
        "  if return_proba:\n",
        "    return sess.run('probabilities:0', feed_dict=feed_dict)\n",
        "  else:\n",
        "    return sess.run('labels:0', feed_dict=feed_dict)"
      ],
      "metadata": {
        "id": "wB3WGApHcCSa"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ahora podemos crear un grafo, establecer la disposición aleatoria y construir el modelo CNN.  "
      ],
      "metadata": {
        "id": "gtr6nLtBdLFD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## definir parámetros.\n",
        "learning_rate = 1e-4\n",
        "random_seed = 123\n",
        "\n",
        "## Crear el grafo:\n",
        "g = tf.Graph()\n",
        "with g.as_default():\n",
        "  tf.compat.v1.set_random_seed(random_seed)\n",
        "  build_cnn()\n",
        "  ## Guardar:\n",
        "  saver = tf.compat.v1.train.Saver()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eSzjlHPZda1S",
        "outputId": "710827ce-95e3-4626-d3f1-04faa6212760"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Construyendo la 1. capa: \n",
            "<tf.Variable 'conv_1/_weights:0' shape=(5, 5, 1, 32) dtype=float32>\n",
            "<tf.Variable 'conv_1/_biases:0' shape=(32,) dtype=float32>\n",
            "Tensor(\"conv_1/Conv2D:0\", shape=(None, 24, 24, 32), dtype=float32)\n",
            "Tensor(\"conv_1/net_pre_activation:0\", shape=(None, 24, 24, 32), dtype=float32)\n",
            "Tensor(\"conv_1/activation:0\", shape=(None, 24, 24, 32), dtype=float32)\n",
            "\n",
            " Construyendo la 2. capa: \n",
            "<tf.Variable 'conv_2/_weights:0' shape=(5, 5, 32, 64) dtype=float32>\n",
            "<tf.Variable 'conv_2/_biases:0' shape=(64,) dtype=float32>\n",
            "Tensor(\"conv_2/Conv2D:0\", shape=(None, 8, 8, 64), dtype=float32)\n",
            "Tensor(\"conv_2/net_pre_activation:0\", shape=(None, 8, 8, 64), dtype=float32)\n",
            "Tensor(\"conv_2/activation:0\", shape=(None, 8, 8, 64), dtype=float32)\n",
            "\n",
            " Contruyendo la 3. capa: \n",
            "<tf.Variable 'fc_3/weights:0' shape=(1024, 1024) dtype=float32>\n",
            "<tf.Variable 'fc_3/biases:0' shape=(1024,) dtype=float32>\n",
            "Tensor(\"fc_3/MatMul:0\", shape=(None, 1024), dtype=float32)\n",
            "Tensor(\"fc_3/BiasAdd:0\", shape=(None, 1024), dtype=float32)\n",
            "Tensor(\"fc_3/activation:0\", shape=(None, 1024), dtype=float32)\n",
            "\n",
            " Construyendo la 4. capa: \n",
            "<tf.Variable 'fc_4/weights:0' shape=(1024, 10) dtype=float32>\n",
            "<tf.Variable 'fc_4/biases:0' shape=(10,) dtype=float32>\n",
            "Tensor(\"fc_4/MatMul:0\", shape=(None, 10), dtype=float32)\n",
            "Tensor(\"fc_4/BiasAdd:0\", shape=(None, 10), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## El siguiente paso consiste en etrenar nuestro modelo CNN. Para ello debemos crear una sesión de tensorFlow e iniciar el grafo, despues llamamos a la función *train*."
      ],
      "metadata": {
        "id": "ZeQFxjLYe8ou"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## division de los datos\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.2, random_state=123)\n",
        "\n",
        "## centrado medio y división por desviacion estandar:\n",
        "mean_vals = np.mean(X_train, axis=0)\n",
        "std_val = np.std(X_train)\n",
        "X_train_centered = (X_train - mean_vals)/std_val\n",
        "X_valid_centered = (X_valid - mean_vals)/std_val\n",
        "X_test_centered = (X_test - mean_vals)/std_val"
      ],
      "metadata": {
        "id": "h0diQFEWg039"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## crear una sesión\n",
        "## y entrenar el modelo CNN:\n",
        "with tf.compat.v1.Session(graph=g) as sess:\n",
        "  # Reshape X_train y X_valid a (num_samples, 784)\n",
        "  X_train_reshaped = X_train.reshape(-1, 784)  # -1 infiere el número de muestras.\n",
        "  X_valid_reshaped = X_valid.reshape(-1, 784)\n",
        "  train(sess, training_set=(X_train_reshaped, y_train), validation_set=(X_valid_reshaped, y_valid), initializer=True, random_seed=123)\n",
        "  save(saver, sess, epoch=20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A-ewd3MAfZeH",
        "outputId": "841394c7-44d7-4de0-b4b7-2aac91fb08f6"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 1 training_loss: 28.1348\n",
            "valid_acc: 0.211\n",
            "epoch: 2 training_loss: 2.1146\n",
            "valid_acc: 0.232\n",
            "epoch: 3 training_loss: 2.0637\n",
            "valid_acc: 0.239\n",
            "epoch: 4 training_loss: 2.0154\n",
            "valid_acc: 0.250\n",
            "epoch: 5 training_loss: 1.9675\n",
            "valid_acc: 0.272\n",
            "epoch: 6 training_loss: 1.9208\n",
            "valid_acc: 0.325\n",
            "epoch: 7 training_loss: 1.8803\n",
            "valid_acc: 0.317\n",
            "epoch: 8 training_loss: 1.8475\n",
            "valid_acc: 0.332\n",
            "epoch: 9 training_loss: 1.7986\n",
            "valid_acc: 0.331\n",
            "epoch: 10 training_loss: 1.7768\n",
            "valid_acc: 0.372\n",
            "epoch: 11 training_loss: 1.7222\n",
            "valid_acc: 0.401\n",
            "epoch: 12 training_loss: 1.6190\n",
            "valid_acc: 0.455\n",
            "epoch: 13 training_loss: 1.4495\n",
            "valid_acc: 0.556\n",
            "epoch: 14 training_loss: 1.1654\n",
            "valid_acc: 0.759\n",
            "epoch: 15 training_loss: 0.8671\n",
            "valid_acc: 0.826\n",
            "epoch: 16 training_loss: 0.6891\n",
            "valid_acc: 0.826\n",
            "epoch: 17 training_loss: 0.5607\n",
            "valid_acc: 0.890\n",
            "epoch: 18 training_loss: 0.4757\n",
            "valid_acc: 0.897\n",
            "epoch: 19 training_loss: 0.4113\n",
            "valid_acc: 0.919\n",
            "epoch: 20 training_loss: 0.3596\n",
            "valid_acc: 0.932\n",
            "guardando el modelo en ./model/\n",
            "modelo guardado\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "del g"
      ],
      "metadata": {
        "id": "BiBMDcRNpB0T"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "# Ahora el mismo entrenamiento con los datos preprocesados:\n",
        "## crear una sesión\n",
        "## y entrenar el modelo CNN:\n",
        "with tf.compat.v1.Session(graph=g) as sess:\n",
        "  # Reshape X_train y X_valid a (num_samples, 784)\n",
        "  X_train_centered_reshaped = X_train_centered.reshape(-1, 784)  # -1 infiere el numero de muestras.\n",
        "  X_valid_centered_reshaped = X_valid_centered.reshape(-1, 784)\n",
        "  train(sess, training_set=(X_train_centered_reshaped, y_train), validation_set=(X_valid_centered_reshaped, y_valid), initializer=True, random_seed=123)\n",
        "  save(saver, sess, epoch=20)\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "fbYS9g2boDgw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Calcular la precisión en el conjunto de prueba\n",
        "## restaurando el modelo guardado\n",
        "\n",
        "## Crear un nuevo grafo\n",
        "## y construir el modelo.\n",
        "g2 = tf.Graph()\n",
        "with g2.as_default():\n",
        "  tf.compat.v1.set_random_seed(random_seed)\n",
        "  ## construir el modelo\n",
        "  build_cnn()\n",
        "  ## Guardar\n",
        "  saver = tf.compat.v1.train.Saver()\n",
        "\n",
        "## Crear una nueva sesión\n",
        "## Y restaurar el modelo.\n",
        "\n",
        "with tf.compat.v1.Session(graph=g2) as sess:\n",
        "  load(saver, sess, epoch=20, path='./model/')\n",
        "  X_test_reshaped = X_test.reshape(-1, 784)\n",
        "  preds = predict(sess, X_test_reshaped, return_proba=False)\n",
        "  print('Test de precisión: %.3f%%'%(100* np.sum(preds == y_test)/len(y_test)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pPF7xSAVq-j9",
        "outputId": "7fc8b0f1-4349-491b-9fb5-3943b4a5166c"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Construyendo la 1. capa: \n",
            "<tf.Variable 'conv_1/_weights:0' shape=(5, 5, 1, 32) dtype=float32>\n",
            "<tf.Variable 'conv_1/_biases:0' shape=(32,) dtype=float32>\n",
            "Tensor(\"conv_1/Conv2D:0\", shape=(None, 24, 24, 32), dtype=float32)\n",
            "Tensor(\"conv_1/net_pre_activation:0\", shape=(None, 24, 24, 32), dtype=float32)\n",
            "Tensor(\"conv_1/activation:0\", shape=(None, 24, 24, 32), dtype=float32)\n",
            "\n",
            " Construyendo la 2. capa: \n",
            "<tf.Variable 'conv_2/_weights:0' shape=(5, 5, 32, 64) dtype=float32>\n",
            "<tf.Variable 'conv_2/_biases:0' shape=(64,) dtype=float32>\n",
            "Tensor(\"conv_2/Conv2D:0\", shape=(None, 8, 8, 64), dtype=float32)\n",
            "Tensor(\"conv_2/net_pre_activation:0\", shape=(None, 8, 8, 64), dtype=float32)\n",
            "Tensor(\"conv_2/activation:0\", shape=(None, 8, 8, 64), dtype=float32)\n",
            "\n",
            " Contruyendo la 3. capa: \n",
            "<tf.Variable 'fc_3/weights:0' shape=(1024, 1024) dtype=float32>\n",
            "<tf.Variable 'fc_3/biases:0' shape=(1024,) dtype=float32>\n",
            "Tensor(\"fc_3/MatMul:0\", shape=(None, 1024), dtype=float32)\n",
            "Tensor(\"fc_3/BiasAdd:0\", shape=(None, 1024), dtype=float32)\n",
            "Tensor(\"fc_3/activation:0\", shape=(None, 1024), dtype=float32)\n",
            "\n",
            " Construyendo la 4. capa: \n",
            "<tf.Variable 'fc_4/weights:0' shape=(1024, 10) dtype=float32>\n",
            "<tf.Variable 'fc_4/biases:0' shape=(10,) dtype=float32>\n",
            "Tensor(\"fc_4/MatMul:0\", shape=(None, 10), dtype=float32)\n",
            "Tensor(\"fc_4/BiasAdd:0\", shape=(None, 10), dtype=float32)\n",
            "cargando el modelo desde ./model/\n",
            "Test de precisión: 93.420%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Podemos seguir entrenando el modelo para conseguir un total de 30 epocas. Esto lo hacemos con *initializer=False*"
      ],
      "metadata": {
        "id": "plGUaANQu9tN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Crear una nueva sesión, restaurar\n",
        "## y entrenar el modelo:\n",
        "with tf.compat.v1.Session(graph=g2) as sess:\n",
        "  load(saver, sess, epoch=20, path='./model/')\n",
        "  train(sess, training_set=(X_train_reshaped, y_train), validation_set=(X_valid_reshaped, y_valid), initializer=False, epoch=10, random_seed=123)\n",
        "  save(saver, sess, epoch=30, path='./model/')\n",
        "  preds = predict(sess, X_test_reshaped, return_proba=False)\n",
        "  print('Test de precisión: %.3f%%'%(100* np.sum(preds == y_test)/len(y_test)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BdPcUggnvRBc",
        "outputId": "2303aec2-b3c2-4d5a-a792-32e8d3a8e08e"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cargando el modelo desde ./model/\n",
            "epoch: 1 training_loss: 0.3203\n",
            "valid_acc: 0.934\n",
            "epoch: 2 training_loss: 0.2846\n",
            "valid_acc: 0.945\n",
            "epoch: 3 training_loss: 0.2501\n",
            "valid_acc: 0.956\n",
            "epoch: 4 training_loss: 0.2306\n",
            "valid_acc: 0.956\n",
            "epoch: 5 training_loss: 0.2125\n",
            "valid_acc: 0.958\n",
            "epoch: 6 training_loss: 0.1916\n",
            "valid_acc: 0.957\n",
            "epoch: 7 training_loss: 0.1773\n",
            "valid_acc: 0.962\n",
            "epoch: 8 training_loss: 0.1696\n",
            "valid_acc: 0.964\n",
            "epoch: 9 training_loss: 0.1652\n",
            "valid_acc: 0.967\n",
            "epoch: 10 training_loss: 0.1547\n",
            "valid_acc: 0.967\n",
            "guardando el modelo en ./model/\n",
            "modelo guardado\n",
            "Test de precisión: 97.030%\n"
          ]
        }
      ]
    }
  ]
}