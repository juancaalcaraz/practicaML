{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPjvI0p95HKFjrsZA9zJfhu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/juancaalcaraz/practicaML/blob/main/CNN_y_RNN_en_TensorFlow1x.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modelo CNN para clasificación de imagenes."
      ],
      "metadata": {
        "id": "rhY5_Cmg4ndZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "E0Zrppn74VwR"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "from tensorflow import keras\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargar el conjunto de datos MNIST\n",
        "(X_train, y_train), (X_test, y_test) = datasets.mnist.load_data()\n",
        "\n",
        "# Imprimir la forma de los datos de entrenamiento\n",
        "print(X_train.shape)  # Salida: (60000, 28, 28)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gjWkM0eO65fs",
        "outputId": "16745a99-d73e-4562-bb4b-ee53832d04a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "(60000, 28, 28)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "shape = (28, 28, 1)\n",
        "model = models.Sequential([\n",
        "    keras.Input(shape=shape),\n",
        "    layers.Conv2D(32, (5, 5), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(64, (5, 5), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(10, activation='softmax')\n",
        "])\n",
        "# Compilar el modelo\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "TcoMJNIU73K3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "21A4u-8p9-Qv",
        "outputId": "8cf6c7f0-d1f7-4128-f937-37d5e35a8894"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ conv2d_4 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │             \u001b[38;5;34m832\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ max_pooling2d_4 (\u001b[38;5;33mMaxPooling2D\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv2d_5 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m64\u001b[0m)            │          \u001b[38;5;34m51,264\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ max_pooling2d_5 (\u001b[38;5;33mMaxPooling2D\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m64\u001b[0m)            │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ flatten_2 (\u001b[38;5;33mFlatten\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)                │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │         \u001b[38;5;34m131,200\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)                  │           \u001b[38;5;34m1,290\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ conv2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">832</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ max_pooling2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv2d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            │          <span style=\"color: #00af00; text-decoration-color: #00af00\">51,264</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ max_pooling2d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ flatten_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)                │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │         <span style=\"color: #00af00; text-decoration-color: #00af00\">131,200</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,290</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m184,586\u001b[0m (721.04 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">184,586</span> (721.04 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m184,586\u001b[0m (721.04 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">184,586</span> (721.04 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "# Definir el callback para guardar el modelo en cada época\n",
        "checkpoint_callback = ModelCheckpoint(\n",
        "    filepath=os.path.join(path, 'cnn_model_checkpoint.h5'),\n",
        "    save_best_only=True,  # Guarda el mejor modelo según el rendimiento en la validación\n",
        "    monitor='val_loss',   # Monitorea la pérdida de validación\n",
        "    mode='min',           # Guarda cuando la pérdida es mínima\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Entrenar el modelo, pasando el callback de checkpoint\n",
        "model.fit(train_data, train_labels,\n",
        "          validation_data=(val_data, val_labels),\n",
        "          epochs=10,\n",
        "          callbacks=[checkpoint_callback])\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "5dx0dv4nAax7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=1, min_lr=0.00001)\n",
        "early_stopping = EarlyStopping(monitor='accuracy', patience=5, restore_best_weights=True, verbose=1)\n",
        "\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=64, callbacks=[reduce_lr, early_stopping])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_AvzkJ9C-L3C",
        "outputId": "ffb365c7-8676-4299-93b2-b489912b233a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 68ms/step - accuracy: 0.7777 - loss: 2.0077 - learning_rate: 0.0010\n",
            "Epoch 2/10\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 64ms/step - accuracy: 0.9641 - loss: 0.1327 - learning_rate: 0.0010\n",
            "Epoch 3/10\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 66ms/step - accuracy: 0.9723 - loss: 0.0950 - learning_rate: 0.0010\n",
            "Epoch 4/10\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 68ms/step - accuracy: 0.9786 - loss: 0.0733 - learning_rate: 0.0010\n",
            "Epoch 5/10\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 65ms/step - accuracy: 0.9796 - loss: 0.0690 - learning_rate: 0.0010\n",
            "Epoch 6/10\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 65ms/step - accuracy: 0.9828 - loss: 0.0644 - learning_rate: 0.0010\n",
            "Epoch 7/10\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 66ms/step - accuracy: 0.9831 - loss: 0.0606 - learning_rate: 0.0010\n",
            "Epoch 8/10\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 64ms/step - accuracy: 0.9850 - loss: 0.0527 - learning_rate: 0.0010\n",
            "Epoch 9/10\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 66ms/step - accuracy: 0.9862 - loss: 0.0476 - learning_rate: 0.0010\n",
            "Epoch 10/10\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 65ms/step - accuracy: 0.9878 - loss: 0.0445 - learning_rate: 0.0010\n",
            "Restoring model weights from the end of the best epoch: 10.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7e01154b2ec0>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluación del modelo\n",
        "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
        "\n",
        "print('Test accuracy:', test_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9_f6KHZLGF9j",
        "outputId": "24cf4333-0593-494b-ae95-5a12281c53c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - accuracy: 0.9836 - loss: 0.0693\n",
            "Test accuracy: 0.9869999885559082\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementar una CNN con la API de bajo nivel de TensorFlow"
      ],
      "metadata": {
        "id": "DOJS94M3kDLH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vamos a crear una función para iterar a travez los minilotes de datos.\n",
        "def batch_generator(X, y, batch_size=64, shuffle=False, random_seed=None):\n",
        "  \"\"\"\n",
        "  Generador de mini lotes de datos.\n",
        "\n",
        "  Esta función devuelve un generador que itera a través de los mini lotes de datos\n",
        "  de entrada `X` y las etiquetas `y`. Es útil para entrenar modelos de Machine Learning\n",
        "  cuando se desea procesar los datos en mini lotes en lugar de todo el conjunto de datos\n",
        "  de una vez.\n",
        "\n",
        "  Parámetros:\n",
        "  X: np.ndarray\n",
        "      Matriz de características de entrada, donde cada fila corresponde a una muestra de datos.\n",
        "  y: np.ndarray\n",
        "      Vector de etiquetas correspondientes a las muestras de `X`.\n",
        "  batch_size: int, opcional (default=64)\n",
        "      El tamaño de cada mini lote. Determina cuántas muestras se devuelven en cada iteración.\n",
        "  shuffle: bool, opcional (default=False)\n",
        "      Si es `True`, las muestras se barajan aleatoriamente antes de la creación de los mini lotes.\n",
        "  random_seed: int, opcional (default=None)\n",
        "      Semilla para el generador de números aleatorios. Se usa solo si `shuffle=True` para asegurar\n",
        "      la reproducibilidad del orden aleatorio.\n",
        "\n",
        "  Yields:\n",
        "  tuple (X_batch, y_batch)\n",
        "      - X_batch: np.ndarray\n",
        "        Un mini lote de datos de entrada, con forma `(batch_size, X.shape[1])`.\n",
        "      - y_batch: np.ndarray\n",
        "        Un mini lote de etiquetas, con forma `(batch_size,)`.\n",
        "\n",
        "  Notas:\n",
        "  - La función no devuelve una lista completa de mini lotes, sino que usa `yield` para\n",
        "    generar los mini lotes uno por uno, lo que la hace adecuada para trabajar con grandes\n",
        "    conjuntos de datos que no caben completamente en memoria.\n",
        "  \"\"\"\n",
        "  idx = np.arange(y.shape[0])\n",
        "  if shuffle:\n",
        "    rng = np.random.RandomState(random_seed)\n",
        "    rng.shuffle(idx)\n",
        "    X = X[idx]\n",
        "    y = y[idx]\n",
        "  for i in range(0, X.shape[0], batch_size):\n",
        "    yield (X[i:i+batch_size, :], y[i:i+batch_size])"
      ],
      "metadata": {
        "id": "0VhBZ5iIkPlW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Para implementar una CNN en TensorFlow definimos dos funciones de envoltorio que harán la construcción de la red más sencilla. Una función de envoltorio para una capa convolucional y otra para la capa completamente conectada."
      ],
      "metadata": {
        "id": "pA2coSPdnAGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def conv_layer(input_tensor, name, kernel_size, n_output_channels, padding_mode='SAME', strides=(1, 1, 1, 1)):\n",
        "  \"\"\"\n",
        "  #################################################################################\n",
        "  # Esta función crea una capa convolucional.                                     #\n",
        "  #################################################################################\n",
        "  parameters:\n",
        "  input_tensor: tf.tensor, tensor de enrtada\n",
        "  name: str Nombre de la capa\n",
        "  kernel_size: int tamaño del kernel\n",
        "  n_output_channels: int Número de canales de salida\n",
        "  padding_mode: str, tipo de padding 'SAME', 'EXPLICIT' o 'VALID' (default='SAME')\n",
        "  strides: tuple (default=(1, 1, 1,))\n",
        "  returns:\n",
        "  tf.tensor: tensor de salida\n",
        "  \"\"\"\n",
        "  with tf.compat.v1.variable_scope(name):\n",
        "    ## obtener n_inputs_channels:\n",
        "    ## Forma del tensor de entrada\n",
        "    ## [batch_size x width x height x channels_in]\n",
        "    input_shape = input_tensor.get_shape().as_list()\n",
        "    n_input_channels = input_shape[-1]\n",
        "\n",
        "    weight_shape = list(kernel_size) + [n_input_channels, n_output_channels]\n",
        "    weights = tf.Variable(tf.random.truncated_normal(weight_shape, stddev=0.01), name='_weights')\n",
        "    print(weights)\n",
        "\n",
        "    biases = tf.Variable(tf.zeros(shape=[n_output_channels]), name='_biases')\n",
        "    print(biases)\n",
        "    conv = tf.nn.conv2d(input=input_tensor, filters=weights, strides=strides, padding=padding_mode)\n",
        "    print(conv)\n",
        "    conv = tf.nn.bias_add(conv, biases, name='net_pre_activation')\n",
        "    print(conv)\n",
        "    conv = tf.nn.relu(conv, name='activation')\n",
        "    print(conv)\n",
        "    return conv"
      ],
      "metadata": {
        "id": "niK_ZR7Bn1fN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# probamos la funcion con un simple gráfico\n",
        "g = tf.Graph()\n",
        "with tf.compat.v1.Graph().as_default() as g:\n",
        "  x = tf.compat.v1.placeholder(tf.float32, shape=[None, 28, 28, 1])\n",
        "  conv_layer(x, name='conv_test', kernel_size=(3, 3), n_output_channels=20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SzcwcVkQu5va",
        "outputId": "a291c49a-b8c3-46f4-cb4f-648e23fa013c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<tf.Variable 'conv_test/_weights:0' shape=(3, 3, 1, 20) dtype=float32>\n",
            "<tf.Variable 'conv_test/_biases:0' shape=(20,) dtype=float32>\n",
            "Tensor(\"conv_test/Conv2D:0\", shape=(None, 28, 28, 20), dtype=float32)\n",
            "Tensor(\"conv_test/net_pre_activation:0\", shape=(None, 28, 28, 20), dtype=float32)\n",
            "Tensor(\"conv_test/activation:0\", shape=(None, 28, 28, 20), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "del g, x"
      ],
      "metadata": {
        "id": "jPre9gUf4KnJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mismo codigo anterior pero optimizado para tensorFlow 2.x"
      ],
      "metadata": {
        "id": "pHc1OFPW-x10"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "@tf.function\n",
        "def conv_layer(input_tensor, name, kernel_size, n_output_channels, padding_mode='same', strides=(1, 1, 1, 1)):\n",
        "    \"\"\"\n",
        "    Esta función crea una capa convolucional en TensorFlow 2.x.\n",
        "\n",
        "    Parameters:\n",
        "    input_tensor: tf.Tensor, tensor de entrada.\n",
        "    name: str, nombre de la capa.\n",
        "    kernel_size: int, tamaño del kernel.\n",
        "    n_output_channels: int, número de canales de salida.\n",
        "    padding_mode: str, tipo de padding ('same' o 'valid'). Default es 'same'.\n",
        "    strides: tuple, pasos (default es (1, 1, 1, 1)).\n",
        "\n",
        "    Returns:\n",
        "    tf.Tensor, tensor resultante de la capa convolucional.\n",
        "    \"\"\"\n",
        "    with tf.name_scope(name):\n",
        "        # Obtener número de canales de entrada\n",
        "        input_shape = input_tensor.shape.as_list()\n",
        "        n_input_channels = input_shape[-1]\n",
        "\n",
        "        # Definir la forma de los pesos\n",
        "        weight_shape = list(kernel_size) + [n_input_channels, n_output_channels]\n",
        "\n",
        "        # Inicializar los pesos y sesgos\n",
        "        weights = tf.Variable(tf.random.normal(weight_shape), name='_weights')\n",
        "        biases = tf.Variable(tf.zeros([n_output_channels]), name='_biases')\n",
        "\n",
        "        # Realizar la convolución\n",
        "        conv = tf.nn.conv2d(input_tensor, filters=weights, strides=strides, padding=padding_mode)\n",
        "\n",
        "        # Sumar el sesgo\n",
        "        conv = tf.nn.bias_add(conv, biases, name='net_pre_activation')\n",
        "\n",
        "        # Aplicar ReLU como función de activación\n",
        "        conv = tf.nn.relu(conv, name='activation')\n",
        "\n",
        "        return conv\n"
      ],
      "metadata": {
        "id": "FJeoXFGN-3ci"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## La siguiente función sirve para definir nuestras capas completamente conectadas."
      ],
      "metadata": {
        "id": "13KCgkBm4Pi0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fc_layer(input_tensor, name, n_output_units, activation_fn=None):\n",
        "  \"\"\"\n",
        "  #################################################################################\n",
        "  # Esta función crea una capa completamente conectada.                           #\n",
        "  #################################################################################\n",
        "  \"\"\"\n",
        "  with tf.name_scope(name):\n",
        "    input_shape = input_tensor.get_shape().as_list()[1:]\n",
        "    n_inputs_units = np.prod(input_shape)\n",
        "    if len(input_shape) > 1:\n",
        "      input_tensor = tf.reshape(input_tensor, shape=(-1, n_inputs_units))\n",
        "    weights_shape = [n_inputs_units, n_output_units]\n",
        "    weights = tf.Variable(initial_value=tf.random.normal(weights_shape), name=\"weights\")\n",
        "    print(weights)\n",
        "    biases = tf.Variable(tf.zeros(shape=[n_output_units]), name='biases')\n",
        "    print(biases)\n",
        "    layer = tf.matmul(input_tensor, weights)\n",
        "    print(layer)\n",
        "    layer = tf.nn.bias_add(layer, biases)\n",
        "    print(layer)\n",
        "    if activation_fn is None:\n",
        "      return layer\n",
        "    layer = activation_fn(layer, name='activation')\n",
        "    print(layer)\n",
        "    return layer"
      ],
      "metadata": {
        "id": "5kyoAZ1r4ZLo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## La función *fc_layers* construye los pesos y los sesgos, los inicializa y luego realiza una multiplicación de matrices mediante la función *tf.matmul*.\n",
        "## La función fc_layers cuenta con tres argumentos obligatorios:\n",
        "1. *input_tensor*: El tensor de entrada.\n",
        "2. *name*: El nombre de la capa, que se utiliza como nombre del alcance.\n",
        "3. *n_output_units*: El número de unidades de salida."
      ],
      "metadata": {
        "id": "bnfxzYeK9Rjw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# probamos la funcion con un simple gráfico\n",
        "g = tf.Graph()\n",
        "with tf.compat.v1.Graph().as_default() as g:\n",
        "  x = tf.compat.v1.placeholder(tf.float32, shape=[None, 28, 28, 1])\n",
        "  fc_layer(x, name='fc_test', n_output_units=32, activation_fn=tf.nn.relu)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lu2_Og5y9weT",
        "outputId": "b0e436e4-a7cb-4546-93df-76adb1ec0713"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<tf.Variable 'fc_test/weights:0' shape=(784, 32) dtype=float32>\n",
            "<tf.Variable 'fc_test/biases:0' shape=(32,) dtype=float32>\n",
            "Tensor(\"fc_test/MatMul:0\", shape=(None, 32), dtype=float32)\n",
            "Tensor(\"fc_test/BiasAdd:0\", shape=(None, 32), dtype=float32)\n",
            "Tensor(\"fc_test/activation:0\", shape=(None, 32), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ahora podemos utilizar estas funciones de envoltorio para construir toda la red convolucional. Definimos una función denominada *buid_cnn* para gestionar la construcción del modelo."
      ],
      "metadata": {
        "id": "FDFnlHT3B2BF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_cnn():\n",
        "  ## Marcadores de posiones para X e y:\n",
        "  tf_x = tf.compat.v1.placeholder(tf.float32, shape=[None, 784], name='tf_x')\n",
        "  tf_y = tf.compat.v1.placeholder(tf.int32, shape=None, name='tf_y')\n",
        "  # Remodelar tf_x en u tensor 4D:\n",
        "  # [batch_size, width, height, 1]\n",
        "  tf_x_image = tf.reshape(tf_x, shape=[-1, 28, 28, 1], name='tf_x_reshaped')\n",
        "  # Codificacion one-hot:\n",
        "  tf_y_onehot = tf.one_hot(indices=tf_y, depth=10, dtype=tf.float32, name='tf_y_onehot')\n",
        "  # 1. capa: Conv_1\n",
        "  print('\\n Construyendo la 1. capa: ')\n",
        "  h1 = conv_layer(tf_x_image, name='conv_1', kernel_size=(5, 5), padding_mode='VALID', n_output_channels=32)\n",
        "  ## Agrupación máxima:\n",
        "  h1_pool = tf.nn.max_pool(h1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
        "  # 2. capa: Conv_2\n",
        "  print('\\n Construyendo la 2. capa: ')\n",
        "  h2  = conv_layer(h1_pool, name='conv_2', kernel_size=(5, 5), padding_mode='VALID', n_output_channels=64)\n",
        "  ## Agrupació máxima:\n",
        "  h2_pool = tf.nn.max_pool(h2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
        "  ## Capa: 3 completamente conectada:\n",
        "  print('\\n Contruyendo la 3. capa: ')\n",
        "  h3 = fc_layer(h2_pool, name='fc_3', n_output_units=1024, activation_fn=tf.nn.relu)\n",
        "  ## Dropout:\n",
        "  keep_prob = tf.compat.v1.placeholder(tf.float32, name='fc_keep_prob')\n",
        "  h3_drop = tf.nn.dropout(h3, rate=1-keep_prob, name='dropout_3')\n",
        "  #h3_drop = tf.nn.dropout(h3, keep_prob=keep_prob, name='dropout_layer')\n",
        "  ## 4. capa: Completamente conectada (Activación líneal)\n",
        "  print('\\n Construyendo la 4. capa: ')\n",
        "  h4 = fc_layer(h3_drop, name='fc_4', n_output_units=10, activation_fn=None)\n",
        "  ## Predicciones:\n",
        "  predictions = {'probabilities': tf.nn.softmax(h4, name='probabilities'),\n",
        "                 'labels': tf.cast(tf.argmax(h4, axis=1), tf.int32, name='labels')}\n",
        "  ## Función de pérdida y optimización\n",
        "  cross_entropy_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=h4, labels= tf_y_onehot), name='cross_entropy_loss')\n",
        "  ## Optimizador:\n",
        "  optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate)\n",
        "  optimizer = optimizer.minimize(cross_entropy_loss, name='train_op')\n",
        "  ## Calcular la precisió de la predicción:\n",
        "  correct_predictions = tf.equal(predictions['labels'], tf_y, name='correct_preds')\n",
        "  accuracy = tf.reduce_mean(tf.cast(correct_predictions, tf.float32), name='accuracy')"
      ],
      "metadata": {
        "id": "xaEHDvW4COaN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ahora defineremos 4 funciones más: *save* y *load* para guardar y cargar los puntos de control del modelo entrenado, *training_set* y *predict* para obtener probabilidades de predicciones o etiquetas de predicción de los datos de prueba."
      ],
      "metadata": {
        "id": "D6njp5fsVpAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "def save(saver, sess, epoch, path='./model/'):\n",
        "  \"\"\"\n",
        "  Función para guardar los pesos, las variables y el estado\n",
        "  del modelo entrenado en tensorFlow 1.x\n",
        "  \"\"\"\n",
        "  if not os.path.isdir(path):\n",
        "    os.mkdir('path')\n",
        "  print(f'guardando el modelo en {path}')\n",
        "  saver.save(sess, os.path.join(path, 'cnn-model.ckpt'), global_step=epoch)\n",
        "  print('modelo guardado')\n"
      ],
      "metadata": {
        "id": "XliRWaZkWdaN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load(saver, sess, path, epoch):\n",
        "  \"\"\"\n",
        "  Función para cargar los pesos, las variables y el estado\n",
        "  del modelo entrenado en tensorFlow 1.x\n",
        "  \"\"\"\n",
        "  print(f'cargando el modelo desde {path}')\n",
        "  saver.restore(sess, os.path.join(path, f'cnn-model.ckpt-{epoch}'))"
      ],
      "metadata": {
        "id": "5VXgFkTrXIfU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(sess, training_set, validation_set=None, initializer= True, epoch=20, shuffle=True, dropout=0.5, random_seed=None):\n",
        "  \"\"\"\n",
        "  Función para entrenar el modelo en tensorFlow 1.x\n",
        "  \"\"\"\n",
        "  X_data = np.array(training_set[0])\n",
        "  y_data = np.array(training_set[1])\n",
        "  training_loss = []\n",
        "  ## Inicializar variables:\n",
        "  if initializer:\n",
        "    sess.run(tf.compat.v1.global_variables_initializer())\n",
        "  np.random.seed(random_seed) #para Shuffle en batch_generator.\n",
        "  for epoch in range(1, epoch + 1):\n",
        "    batch_gen = batch_generator(X_data, y_data, shuffle=shuffle)\n",
        "    avg_loss = 0.0\n",
        "    for i, (batch_x, batch_y) in enumerate(batch_gen):\n",
        "      batch_x = batch_x.reshape(batch_x.shape[0], -1)\n",
        "      feed_dict = {'tf_x: 0': batch_x, 'tf_y: 0': batch_y, 'fc_keep_prob: 0':dropout}\n",
        "      loss, _ = sess.run(['cross_entropy_loss:0', 'train_op'], feed_dict=feed_dict)\n",
        "      avg_loss += loss\n",
        "    training_loss.append(avg_loss/(i+1))\n",
        "    print(f'epoch: {epoch} training_loss: {training_loss[-1]:.4f}')\n",
        "    if validation_set is not None:\n",
        "      feed = {'tf_x:0': validation_set[0], 'tf_y:0': validation_set[1], 'fc_keep_prob:0': 1.0}\n",
        "      valid_acc = sess.run('accuracy:0', feed_dict=feed)\n",
        "      print(f'valid_acc: {valid_acc:.3f}')\n",
        "    else :\n",
        "      print()"
      ],
      "metadata": {
        "id": "REqJy_CVYVc-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(sess, X_test, return_proba=False):\n",
        "  \"\"\"\n",
        "  Función para realizar predicciones en tensorFlow 1.x\n",
        "  \"\"\"\n",
        "  feed_dict = {'tf_x:0': X_test, 'fc_keep_prob:0': 1.0}\n",
        "  if return_proba:\n",
        "    return sess.run('probabilities:0', feed_dict=feed_dict)\n",
        "  else:\n",
        "    return sess.run('labels:0', feed_dict=feed_dict)"
      ],
      "metadata": {
        "id": "wB3WGApHcCSa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ahora podemos crear un grafo, establecer la disposición aleatoria y construir el modelo CNN.  "
      ],
      "metadata": {
        "id": "gtr6nLtBdLFD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## definir parámetros.\n",
        "learning_rate = 1e-4\n",
        "random_seed = 123\n",
        "\n",
        "## Crear el grafo:\n",
        "g = tf.Graph()\n",
        "with g.as_default():\n",
        "  tf.compat.v1.set_random_seed(random_seed)\n",
        "  build_cnn()\n",
        "  ## Guardar:\n",
        "  saver = tf.compat.v1.train.Saver()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eSzjlHPZda1S",
        "outputId": "19d92910-c569-4d73-a62a-03680613ac56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Construyendo la 1. capa: \n",
            "<tf.Variable 'conv_1/_weights:0' shape=(5, 5, 1, 32) dtype=float32>\n",
            "<tf.Variable 'conv_1/_biases:0' shape=(32,) dtype=float32>\n",
            "Tensor(\"conv_1/Conv2D:0\", shape=(None, 24, 24, 32), dtype=float32)\n",
            "Tensor(\"conv_1/net_pre_activation:0\", shape=(None, 24, 24, 32), dtype=float32)\n",
            "Tensor(\"conv_1/activation:0\", shape=(None, 24, 24, 32), dtype=float32)\n",
            "\n",
            " Construyendo la 2. capa: \n",
            "<tf.Variable 'conv_2/_weights:0' shape=(5, 5, 32, 64) dtype=float32>\n",
            "<tf.Variable 'conv_2/_biases:0' shape=(64,) dtype=float32>\n",
            "Tensor(\"conv_2/Conv2D:0\", shape=(None, 8, 8, 64), dtype=float32)\n",
            "Tensor(\"conv_2/net_pre_activation:0\", shape=(None, 8, 8, 64), dtype=float32)\n",
            "Tensor(\"conv_2/activation:0\", shape=(None, 8, 8, 64), dtype=float32)\n",
            "\n",
            " Contruyendo la 3. capa: \n",
            "<tf.Variable 'fc_3/weights:0' shape=(1024, 1024) dtype=float32>\n",
            "<tf.Variable 'fc_3/biases:0' shape=(1024,) dtype=float32>\n",
            "Tensor(\"fc_3/MatMul:0\", shape=(None, 1024), dtype=float32)\n",
            "Tensor(\"fc_3/BiasAdd:0\", shape=(None, 1024), dtype=float32)\n",
            "Tensor(\"fc_3/activation:0\", shape=(None, 1024), dtype=float32)\n",
            "\n",
            " Construyendo la 4. capa: \n",
            "<tf.Variable 'fc_4/weights:0' shape=(1024, 10) dtype=float32>\n",
            "<tf.Variable 'fc_4/biases:0' shape=(10,) dtype=float32>\n",
            "Tensor(\"fc_4/MatMul:0\", shape=(None, 10), dtype=float32)\n",
            "Tensor(\"fc_4/BiasAdd:0\", shape=(None, 10), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## El siguiente paso consiste en etrenar nuestro modelo CNN. Para ello debemos crear una sesión de tensorFlow e iniciar el grafo, despues llamamos a la función *train*."
      ],
      "metadata": {
        "id": "ZeQFxjLYe8ou"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## division de los datos\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.2, random_state=123)\n",
        "\n",
        "## centrado medio y división por desviacion estandar:\n",
        "mean_vals = np.mean(X_train, axis=0)\n",
        "std_val = np.std(X_train)\n",
        "X_train_centered = (X_train - mean_vals)/std_val\n",
        "X_valid_centered = (X_valid - mean_vals)/std_val\n",
        "X_test_centered = (X_test - mean_vals)/std_val"
      ],
      "metadata": {
        "id": "h0diQFEWg039"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "## entrenamiento con los datos preprocesados.\n",
        "## crear una sesión\n",
        "## y entrenar el modelo CNN:\n",
        "with tf.compat.v1.Session(graph=g) as sess:\n",
        "  # Reshape X_train y X_valid a (num_samples, 784)\n",
        "  X_train_centered_reshaped = X_train_centered.reshape(-1, 784)  # -1 infiere el numero de muestras.\n",
        "  X_valid_centered_reshaped = X_valid_centered.reshape(-1, 784)\n",
        "  train(sess, training_set=(X_train_centered_reshaped, y_train), validation_set=(X_valid_centered_reshaped, y_valid), initializer=True, random_seed=123)\n",
        "  save(saver, sess, epoch=20)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fbYS9g2boDgw",
        "outputId": "1efec26b-2fed-4126-b5bd-31e4d1230152"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 1 training_loss: 1.0700\n",
            "valid_acc: 0.940\n",
            "epoch: 2 training_loss: 0.3231\n",
            "valid_acc: 0.961\n",
            "epoch: 3 training_loss: 0.2163\n",
            "valid_acc: 0.972\n",
            "epoch: 4 training_loss: 0.1609\n",
            "valid_acc: 0.978\n",
            "epoch: 5 training_loss: 0.1309\n",
            "valid_acc: 0.980\n",
            "epoch: 6 training_loss: 0.1091\n",
            "valid_acc: 0.982\n",
            "epoch: 7 training_loss: 0.0919\n",
            "valid_acc: 0.983\n",
            "epoch: 8 training_loss: 0.0859\n",
            "valid_acc: 0.983\n",
            "epoch: 9 training_loss: 0.0741\n",
            "valid_acc: 0.984\n",
            "epoch: 10 training_loss: 0.0686\n",
            "valid_acc: 0.983\n",
            "epoch: 11 training_loss: 0.0615\n",
            "valid_acc: 0.985\n",
            "epoch: 12 training_loss: 0.0567\n",
            "valid_acc: 0.986\n",
            "epoch: 13 training_loss: 0.0553\n",
            "valid_acc: 0.987\n",
            "epoch: 14 training_loss: 0.0483\n",
            "valid_acc: 0.988\n",
            "epoch: 15 training_loss: 0.0462\n",
            "valid_acc: 0.989\n",
            "epoch: 16 training_loss: 0.0408\n",
            "valid_acc: 0.990\n",
            "epoch: 17 training_loss: 0.0398\n",
            "valid_acc: 0.989\n",
            "epoch: 18 training_loss: 0.0361\n",
            "valid_acc: 0.989\n",
            "epoch: 19 training_loss: 0.0347\n",
            "valid_acc: 0.990\n",
            "epoch: 20 training_loss: 0.0313\n",
            "valid_acc: 0.991\n",
            "guardando el modelo en ./model/\n",
            "modelo guardado\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Calcular la precisión en el conjunto de prueba\n",
        "## restaurando el modelo guardado\n",
        "\n",
        "## Crear un nuevo grafo\n",
        "## y construir el modelo.\n",
        "g2 = tf.Graph()\n",
        "with g2.as_default():\n",
        "  tf.compat.v1.set_random_seed(random_seed)\n",
        "  ## construir el modelo\n",
        "  build_cnn()\n",
        "  ## Guardar\n",
        "  saver = tf.compat.v1.train.Saver()\n",
        "\n",
        "## Crear una nueva sesión\n",
        "## Y restaurar el modelo.\n",
        "\n",
        "with tf.compat.v1.Session(graph=g2) as sess:\n",
        "  load(saver, sess, epoch=20, path='./model/')\n",
        "  X_test_centered_reshaped = X_test_centered.reshape(-1, 784)\n",
        "  #X_test_reshaped = X_test.reshape(-1, 784)\n",
        "  preds = predict(sess, X_test_centered_reshaped, return_proba=False)\n",
        "  print('Test de precisión: %.3f%%'%(100* np.sum(preds == y_test)/len(y_test)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pPF7xSAVq-j9",
        "outputId": "06be8a6a-0391-4da0-8c73-6bf04fcc3177"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Construyendo la 1. capa: \n",
            "<tf.Variable 'conv_1/_weights:0' shape=(5, 5, 1, 32) dtype=float32>\n",
            "<tf.Variable 'conv_1/_biases:0' shape=(32,) dtype=float32>\n",
            "Tensor(\"conv_1/Conv2D:0\", shape=(None, 24, 24, 32), dtype=float32)\n",
            "Tensor(\"conv_1/net_pre_activation:0\", shape=(None, 24, 24, 32), dtype=float32)\n",
            "Tensor(\"conv_1/activation:0\", shape=(None, 24, 24, 32), dtype=float32)\n",
            "\n",
            " Construyendo la 2. capa: \n",
            "<tf.Variable 'conv_2/_weights:0' shape=(5, 5, 32, 64) dtype=float32>\n",
            "<tf.Variable 'conv_2/_biases:0' shape=(64,) dtype=float32>\n",
            "Tensor(\"conv_2/Conv2D:0\", shape=(None, 8, 8, 64), dtype=float32)\n",
            "Tensor(\"conv_2/net_pre_activation:0\", shape=(None, 8, 8, 64), dtype=float32)\n",
            "Tensor(\"conv_2/activation:0\", shape=(None, 8, 8, 64), dtype=float32)\n",
            "\n",
            " Contruyendo la 3. capa: \n",
            "<tf.Variable 'fc_3/weights:0' shape=(1024, 1024) dtype=float32>\n",
            "<tf.Variable 'fc_3/biases:0' shape=(1024,) dtype=float32>\n",
            "Tensor(\"fc_3/MatMul:0\", shape=(None, 1024), dtype=float32)\n",
            "Tensor(\"fc_3/BiasAdd:0\", shape=(None, 1024), dtype=float32)\n",
            "Tensor(\"fc_3/activation:0\", shape=(None, 1024), dtype=float32)\n",
            "\n",
            " Construyendo la 4. capa: \n",
            "<tf.Variable 'fc_4/weights:0' shape=(1024, 10) dtype=float32>\n",
            "<tf.Variable 'fc_4/biases:0' shape=(10,) dtype=float32>\n",
            "Tensor(\"fc_4/MatMul:0\", shape=(None, 10), dtype=float32)\n",
            "Tensor(\"fc_4/BiasAdd:0\", shape=(None, 10), dtype=float32)\n",
            "cargando el modelo desde ./model/\n",
            "Test de precisión: 99.180%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Podemos seguir entrenando el modelo para conseguir un total de 30 epocas. Esto lo hacemos con *initializer=False*"
      ],
      "metadata": {
        "id": "plGUaANQu9tN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Crear una nueva sesión, restaurar\n",
        "## y entrenar el modelo:\n",
        "with tf.compat.v1.Session(graph=g2) as sess:\n",
        "  load(saver, sess, epoch=20, path='./model/')\n",
        "  train(sess, training_set=(X_train_centered_reshaped, y_train), validation_set=(X_valid_centered_reshaped, y_valid), initializer=False, epoch=10, random_seed=123)\n",
        "  save(saver, sess, epoch=30, path='./model/')\n",
        "  preds = predict(sess, X_test_centered_reshaped, return_proba=False)\n",
        "  print('Test de precisión: %.3f%%'%(100* np.sum(preds == y_test)/len(y_test)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BdPcUggnvRBc",
        "outputId": "382727ee-c097-4286-9ecd-6d116af77f24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cargando el modelo desde ./model/\n",
            "epoch: 1 training_loss: 0.0307\n",
            "valid_acc: 0.990\n",
            "epoch: 2 training_loss: 0.0276\n",
            "valid_acc: 0.990\n",
            "epoch: 3 training_loss: 0.0263\n",
            "valid_acc: 0.990\n",
            "epoch: 4 training_loss: 0.0263\n",
            "valid_acc: 0.990\n",
            "epoch: 5 training_loss: 0.0236\n",
            "valid_acc: 0.990\n",
            "epoch: 6 training_loss: 0.0234\n",
            "valid_acc: 0.991\n",
            "epoch: 7 training_loss: 0.0243\n",
            "valid_acc: 0.989\n",
            "epoch: 8 training_loss: 0.0214\n",
            "valid_acc: 0.991\n",
            "epoch: 9 training_loss: 0.0211\n",
            "valid_acc: 0.991\n",
            "epoch: 10 training_loss: 0.0203\n",
            "valid_acc: 0.991\n",
            "guardando el modelo en ./model/\n",
            "modelo guardado\n",
            "Test de precisión: 99.130%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modelado de datos sequenciales mediante redes neuronales recurrentes. **RNN**"
      ],
      "metadata": {
        "id": "RF_wdooB8uEi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Proyecto uno: crear un análisis de sentimientos de las críticas de las películas IMDb con RNN multicapa"
      ],
      "metadata": {
        "id": "BOhjAWQl_Ne1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## En está seccion implementaremos una RNN multicapa para el análisis de sentimientos mediante una arquitectura *many to one*"
      ],
      "metadata": {
        "id": "sybxT2x5AgUo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyprind"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7pWvpRLWBRro",
        "outputId": "cab81b2c-0916-444f-ef3a-c10aa1f47067"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyprind in /usr/local/lib/python3.10/dist-packages (2.11.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pyprind\n",
        "import pandas as pd\n",
        "from string import punctuation\n",
        "import re\n",
        "import numpy as np\n",
        "\n",
        "df = pd.read_csv('movie_data.csv', encoding='utf-8')\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "ngP9z5vMBZq6",
        "outputId": "388d159e-799e-4a1d-af20-b7d788a573d7"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                              review  sentiment\n",
              "0  In 1974, the teenager Martha Moxley (Maggie Gr...          1\n",
              "1  OK... so... I really like Kris Kristofferson a...          0\n",
              "2  ***SPOILER*** Do not read this, if you think a...          0\n",
              "3  hi for all the people who have seen this wonde...          1\n",
              "4  I recently bought the DVD, forgetting just how...          0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-057c4b74-29f6-448d-9241-e93bc0c2d22e\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>In 1974, the teenager Martha Moxley (Maggie Gr...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>OK... so... I really like Kris Kristofferson a...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>***SPOILER*** Do not read this, if you think a...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>hi for all the people who have seen this wonde...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>I recently bought the DVD, forgetting just how...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-057c4b74-29f6-448d-9241-e93bc0c2d22e')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-057c4b74-29f6-448d-9241-e93bc0c2d22e button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-057c4b74-29f6-448d-9241-e93bc0c2d22e');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-4f3176af-97ad-4cae-a8f4-d35689d3a4d5\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-4f3176af-97ad-4cae-a8f4-d35689d3a4d5')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-4f3176af-97ad-4cae-a8f4-d35689d3a4d5 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 50000,\n  \"fields\": [\n    {\n      \"column\": \"review\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 49582,\n        \"samples\": [\n          \"Every time I think about this film I feel physically ill. To read such a great book and later discover there's a film of it was a great feeling. Years later and imagine my joy at switching on the sci-fi channel and finding it starts in just 5mins!!! Up go the titles and then uggg. If just a couple of things had changed OK. Everything is changed. Numerous characters are removed entirely new rubbish ones are added. The main hero is shrunk and de-aged by about 30 years, and hilariously his girlfriend/wife is now his mother! Even the dog is reduced to sub-lassie capabilities. This is truly appalling cinema at its absolute worst. I would quite happily remove my own toenails with pliers rather than sit through another horrific viewing, and I urge anyone thinking of watching this - please don't. If you own a copy burn it now, right now and think how much better your life would have been had this celluloid insult never occurred.\",\n          \"As with all environmentally aware films from the 1970s SOYLENT GREEN has a rather cheesy view of what ecological meltdown is . Overpopulation means there`s too many people to feed ? I was under the impression that famines were caused by either war or failed economic policies . Stalin`s policy in the Soviet Union in the 1930s left millions dead because of famine and to this day the greatest man made tragedy was Mao`s rural policy in China which led over 30 million starvation deaths in the 1950s . And let`s not forget the great famines in the horn of Africa in the 1980s and 90s which were to do with conflicts not overpopulation . You might like to also consider that two of the most heavily populated areas on Earth , Hong Kong and Macau , have never suffered a famine in modern times . Likewise the expansion of shanty towns around cities as seen here isn`t strictly down to overpopulation - it`s down to economic factors where people flock to cities to find better paid work than in the countryside ( It`s a symptom of industrial progress - not of too many births ) so the image of the streets of New York city being too congested to walk through and of having people sleep in stairwells is somewhat laughable<br /><br />But don`t be fooled into thinking SOYLENT GREEN is a pile of corny tree hugging crap because I consider this to be the best ecological film of the<br /><br />70s . It plays on the contempary audience`s knowledge of the world where Sol and Thorn are beside themselves with joy at finding fruit , brandy and fresh meat . Thorn gasps in amazement at having ice in his whisky , puffs on a cigarette and delivers the classic line \\\" If I could afford it I`d smoke two , maybe three of these a day \\\" . But it`s the visage of the euthanasia chamber that`s memorable as Thorn gazes at the images of wild animals , flowers , running water and snow covered mountains , a world Thorn`s generation has never known . This is a very haunting scene which makes SOYLENT GREEN a very memorable film , combined with the fact it features the final screen appearance of Edward G Robinson as the wise old Jew Sol Roth\",\n          \"Yah. I know. It has the name \\\"Sinatra\\\" in the title, so how bad can it be? Well, it's bad, trust me! I rented this thinking it was some movie I missed in the theaters. It's not. It's some garbage \\\"movie\\\" made by the folks at Showtime (cable station). Geez, these cable stations make a few bucks they think they can make whatever garbage movies they want! It's not good. I am as big a Sinatra fan as any sane man, but this movie was just dumb. Boring. Dull. Unfunny. Uninteresting. The only redeeming quality is that (assuming they did stick to the facts) you do learn about what happened to the captors of Frank Jr. Otherwise it's just a stupid film.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sentiment\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "import re\n",
        "from collections import Counter\n",
        "import pyprind\n",
        "\n",
        "# Definir puntuación\n",
        "punctuation = string.punctuation\n",
        "\n",
        "# Inicializar el contador de palabras\n",
        "counts = Counter()\n",
        "\n",
        "# Barra de progreso para contar las ocurrencias\n",
        "pbar = pyprind.ProgBar(len(df['review']), title='Contando ocurrencia de palabras')\n",
        "\n",
        "# Preprocesar el texto\n",
        "for i, review in enumerate(df['review']):\n",
        "    # Eliminar signos de puntuación (al final o en medio de las palabras)\n",
        "    review = re.sub(r'[^\\w\\s]', '', review)  # Elimina signos de puntuación (excepto espacios)\n",
        "\n",
        "    # Convertir a minúsculas y actualizar el contador\n",
        "    text = review.lower()\n",
        "    pbar.update()\n",
        "    counts.update(text.split())\n",
        "\n",
        "# Obtener las palabras más frecuentes\n",
        "word_counts = sorted(counts, key=counts.get, reverse=True)\n",
        "\n",
        "# Mostrar las 5 palabras más frecuentes\n",
        "print(word_counts[:5])\n",
        "\n",
        "# Crear el diccionario para mapear palabras a enteros\n",
        "word_to_int = {word: ii for ii, word in enumerate(word_counts, 1)}\n",
        "\n",
        "# Lista para almacenar las reseñas mapeadas\n",
        "mapped_reviews = []\n",
        "\n",
        "# Barra de progreso para mapear las reseñas\n",
        "pbar = pyprind.ProgBar(len(df['review']), title='mapeando reviews a enteros')\n",
        "\n",
        "# Mapear las reseñas a números\n",
        "for review in df['review']:\n",
        "    # Convertir la reseña a minúsculas\n",
        "    review = review.lower()\n",
        "\n",
        "    # Dividir la reseña en palabras y reemplazar cada palabra con su índice correspondiente\n",
        "    mapped_reviews.append([word_to_int.get(word, 0) for word in review.split()])  # Usar 0 como valor predeterminado\n",
        "\n",
        "    # Actualizar la barra de progreso\n",
        "    pbar.update()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fP1VOV15P_Mk",
        "outputId": "0faeffe6-718d-4707-cd25-e104d9bde898"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Contando ocurrencia de palabras\n",
            "0% [##############################] 100% | ETA: 00:00:00\n",
            "Total time elapsed: 00:00:09\n",
            "mapeando reviews a enteros\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['the', 'and', 'a', 'of', 'to']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "0% [##############################] 100% | ETA: 00:00:00\n",
            "Total time elapsed: 00:00:06\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Nota:\n",
        "1. [^\\w\\s] significa \"cualquier cosa que no sea una letra, número o espacio\" y re.sub() lo reemplaza con una cadena vacía (''), eliminando los signos de puntuación.\n",
        "2. word_to_int.get(word, 0): Esto asegura que si alguna palabra no se encuentra en el diccionario, en lugar de lanzar un KeyError, se asigna un valor predeterminado (en este caso, 0). Esto es útil para evitar errores si hay palabras que no están en el diccionario word_to_int\n",
        "3.  Usar el método .get() con un valor predeterminado (como 0) asegura que el código no se detenga y continúe procesando las reseñas sin lanzar errores."
      ],
      "metadata": {
        "id": "4_gdW4TKRTKz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ya convertimos secuencias de palabras en secuencias de enteros. Ahora para poder alimentar nuestra RNN debemos hacer que todas las secuencias tengan la misma longitud. Para ello definimos un parámetro denominado *sequence_length* que establecemos en 200. las sequencias que tengan menos de 200 serán rellendas y las que tengan más se cortan de manera que solo se utilicen las últimas 200 correspondientes.\n",
        "## Podemos implementar este paso de preprocesamiento en dos pasos:\n",
        "1. Crear una matriz de ceros, donde cada fila corresponde a una secuencia de tamaño 200.\n",
        "2. Rellenar el índice de palabras en cada secuencia desde la parte derecha de la matriz."
      ],
      "metadata": {
        "id": "iid7QrjjR9GU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Definir secuencias con la misma longitud\n",
        "## sequence_length se puede configurar\n",
        "## con otros valores\n",
        "sequence_length = 200\n",
        "sequences = np.zeros((len(mapped_reviews), sequence_length), dtype=int)\n",
        "\n",
        "for i, row in enumerate(mapped_reviews):\n",
        "  review_arr = np.array(row)\n",
        "  sequences[i, -len(row):] = review_arr[-sequence_length:]"
      ],
      "metadata": {
        "id": "sA24PgglZ-ZV"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Como el conjunto de enrtenamiennto ya esta mezclado solo debemos dividirlo para el enrtenamiento y para la prueba"
      ],
      "metadata": {
        "id": "vEQwl41YccUI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = sequences[:25000, :]\n",
        "y_train = df.loc[:25000, 'sentiment'].values\n",
        "X_test = sequences[25000:, :]\n",
        "y_test = df.loc[25000:, 'sentiment'].values"
      ],
      "metadata": {
        "id": "EUQu9OX4cvX0"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Definimos una función de ayuda que rompa en trozos los datos proporcionados y devuelva un generador para iterar a travez de estos trozos."
      ],
      "metadata": {
        "id": "26fBC2kNcz-5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(123)\n",
        "\n",
        "## Definir una función para generar minilotes:\n",
        "def create_batch_generator(x, y=None, batch_size=64):\n",
        "  n_batches = len(x)// batch_size\n",
        "  x = x[:n_batches*batch_size]\n",
        "  if y is not None:\n",
        "    y = y[:n_batches*batch_size]\n",
        "  for ii in range(0, len(x), batch_size):\n",
        "    if y is not None:\n",
        "      yield x[ii:ii+batch_size], y[ii:ii+batch_size]\n",
        "    else:\n",
        "      yield x[ii:ii+batch_size]"
      ],
      "metadata": {
        "id": "KjT-44v3dHqj"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ahora utilizaremos *Embedding* para mapear cada palabra en un vector de tamaño fijo. Podemos utilizar vectores de tamaño limitado para representar un número infinito de números reales.\n",
        "## Una vez que tengamos el número de palabras únicas *unique_words*, podemos elegir el tamaño de los vectores de incrustración para que sea mucho más pequeño que el número de palabras únicas (*embedding_size* << *unique_words*)\n",
        "## Ventajas de la técnia Embedding\n",
        "1. Una reducción de la dimensionalidad del espacio de características para disminuir el efecto de la maldición de la dimensionalidad.\n",
        "2. La extracción de características salientes, pues la capa de inscrustración en una red neuronal es entrenable.\n",
        "\n",
        "## Para crear una capa de incrustración: Si tenemos *tf_x* como capa de entrada donde los índices de vocabulario correspondientes se proporcionan con el tipo *tf.int32*, la creación del Embedding se puede realizar de la siguiente manera\n",
        "1. Creamos una matriz de tamaño [n_words x embedding_size] como una variable de tensor e inicializamos sus elementos de forma aleatoria con números flotantes entre [-1, 1]:\n",
        "\n",
        "\n",
        "```\n",
        "# Matriz del embedding\n",
        "embedding = tf.Variable(tf.random_uniform(shape=(n_words, embedding_size), min_val=-1, max_val=1)\n",
        "    )\n",
        "```\n",
        "2. Despúes, utilizamos la función *tf.nn.embedding_lookup* para buscar la fila en la matriz de icrustración asociada con cada elemento de *tf_x*:\n",
        "\n",
        "\n",
        "```\n",
        "## tf.nn.embedding_lookup Requiere dos elementos\n",
        "## el tensor de incrustración\n",
        "## y los ID de búsqueda.\n",
        "\n",
        "embed_x = tf.nn.embedding_lookup(embedding, tf_x)\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "HXCwddWDe43p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Contruir un modelo RNN\n",
        "### Implementaremos una clase *SentimentRNN* que tiene los siguientes métodos:\n",
        "\n",
        "\n",
        "*   Un constructor para ajustar todos los parámetros del modelo y despúes crear un gráfico de cálculo y llamar el método *self.build* para construir el modelo RNN multicapa.\n",
        "*   Un método *build* que declara tres marcadores de posición para los datos de entrada y la probabilidad mantenida para la configuración *Dropout* de la capa oculta. Tras realizar esta declaración, se crea una capa de incrustración como entrada.\n",
        "* Un método *train* que crea una sesió de TensorFlow para iniciar el grafo de cálculo, itera a través de los minilotes de datos y se ejecuta durante un número fijo de épocas, para minimizar la función de coste definida en el grafo. Este método también guarda el modelo cada 10 épocas para su control.\n",
        "* Un método *predict* que crea una ueva sesión, restablece el último punto de control guardado durante el proceso de entrenamiento y muestra las predicciones para los datos de prueba.  \n",
        "\n"
      ],
      "metadata": {
        "id": "aAdb2PCsu-BU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Nota:\n",
        "#       Este código ya no funciona en tensorFlow 2.x algunos modulos necesarios fueron removidos.\n",
        "\"\"\"\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "## Definimos la clase SentimentRNN\n",
        "class SentimentRNN(object):\n",
        "  ## Definimos el constructor\n",
        "  def __init__(self, n_words: int , seq_len: int = 200, lstm_size: int = 256,\n",
        "               num_layers: int = 1, batch_size: int =64, learning_rate: float = 0.0001, embedding_size: int = 200):\n",
        "\n",
        "     n_words: se debe establecer igual al número de palabras únicas + 1\n",
        "             se utuliza mientras se crea la capa de incrustación\n",
        "             con el hiperparámetro embed_size.\n",
        "    seq_len: debe ajustarse según la longitud de las secuencias que\n",
        "             han sido creadas en el preprocesamiento.\n",
        "    lstm_size: Determina el número de únidades ocultas en cada\n",
        "               capa de la RNN.\n",
        "\n",
        "    self.n_words = n_words\n",
        "    self.seq_len = seq_len\n",
        "    self.lstm_size = lstm_size\n",
        "    self.num_layers = num_layers\n",
        "    self.batch_size = batch_size\n",
        "    self.learning_rate = learning_rate\n",
        "    self.embedding_size = embedding_size\n",
        "    # Construir el grafo\n",
        "    self.g = tf.Graph()\n",
        "    with self.g.as_default():\n",
        "      tf.compat.v1.set_random_seed(123)\n",
        "      self.build()\n",
        "      self.saver = tf.compat.v1.train.Saver()\n",
        "      self.init_op = tf.compat.v1.global_variables_initializer()\n",
        "\n",
        "  def build(self):\n",
        "    ## definir los marcadores de posición\n",
        "    tf_x = tf.compat.v1.placeholder(tf.int32, shape=(self.batch_size, self.seq_len), name='tf_x')\n",
        "    tf_y = tf.compat.v1.placeholder(tf.float32, shape=(self.batch_size), name='tf_y')\n",
        "    tf_keepprob = tf.compat.v1.placeholder(tf.float32, name='tf_keepprob')\n",
        "    ## Crear una capa de incrustación:\n",
        "    embedding = tf.Variable(tf.random.uniform((self.n_words, self.embedding_size), -1, 1), name='embedding')\n",
        "    embed_x = tf.nn.embedding_lookup(embedding, tf_x, name='embeded_x')\n",
        "\n",
        "    ## definir las celdas LSTM y mantenerlas juntas:\n",
        "    cells = tf.contrib.rnn.MultiRNNCell(\n",
        "        [tf.compat.v1.contrib.rnn.DropoutWrapper(\n",
        "            tf.compat.v1.contrib.rnn.BasicLSTMCell(self.lstm_size),\n",
        "            output_keep_prob=tf_keepprob\n",
        "        ) for i in range(self.num_layers)]\n",
        "    )\n",
        "    ## Definir el estado inicial.\n",
        "    self.initial_state = cells.zero_state(self.batch_size, tf.float32)\n",
        "    print('<<Estado inicial: >>', self.initial_state)\n",
        "    lstm_outputs, self.final_state = tf.nn.dynamic_rnn(cells, embed_x, initial_state=self.initial_state)\n",
        "    ## Nota: lstm_ouputs modela:\n",
        "    ## [batch_size, max_time, cells.output_size]\n",
        "    print('\\n << lstm_output >>', lstm_outputs)\n",
        "    print('\\n << estado final >>', self.final_state)\n",
        "\n",
        "    logits = tf.layers.dense(\n",
        "        inputs=lstm_outputs[:, -1],\n",
        "        units=1, activation=None, name='logits'\n",
        "    )\n",
        "    logits = tf.squeeze(logits, name='logits_squeezed')\n",
        "    print('\\n << logits >>', logits)\n",
        "\n",
        "    y_proba = tf.nn.sigmoid(logits, name='probabilities')\n",
        "    predictions = {\n",
        "        'probabilities': y_proba,\n",
        "        'labels': tf.cast(tf.round(y_proba), tf.int32, name='labels')\n",
        "    }\n",
        "    print('\\n << predicciones >>', predictions)\n",
        "    ## Definir la funcion de coste:\n",
        "    cost = tf.reduce_mean(\n",
        "        tf.nn.sigmoid_cross_entropy_with_logits(labels=tf_y, logits=logits),\n",
        "        name='cost'\n",
        "    )\n",
        "    ## Definir el Optimizador\n",
        "    optimizer = tf.compat.v1.train.AdamOptimizer(self.learning_rate)\n",
        "    train_op = optimizer.minimize(cost, name='train_op')\n",
        "\n",
        "  def train(self, X_train, y_train, num_epochs):\n",
        "    with tf.compat.v1.Session(graph=self.g) as sess:\n",
        "      sess.run(self.init_op)\n",
        "      iteration = 1\n",
        "      for epoch in range(num_epochs):\n",
        "        state = sess.run(self.initial_state)\n",
        "        for batch_x, batch_y in create_batch_generator(X_train, y_train, self.batch_size):\n",
        "          feed = {'tf_x:0': batch_x,\n",
        "                  'tf_y:0': batch_y,\n",
        "                  'tf_keepproba:0': 0.5,\n",
        "                  self.initial_state: state\n",
        "              }\n",
        "          loss, _, state = sess.run(\n",
        "              ['cost:0', 'train_op', self.final_state],\n",
        "              feed_dict=feed\n",
        "          )\n",
        "          if iteration % 20 == 0:\n",
        "            print(f'Epoca: {epoch+1} iteración: {iteration} loss: {loss}')\n",
        "          iteration += 1\n",
        "        if (epoch+1)%10 == 0:\n",
        "          self.saver.save(sess, 'model/sentiment-%d.ckpt'%(epoch+1))\n",
        "  def predict(self, X_data, return_proba=False):\n",
        "    preds = []\n",
        "    with tf.compat.v1.Session(graph=self.g) as sess:\n",
        "      self.saver.restore(sess, tf.train.latest_checkpoint('./model/'))\n",
        "      test_state = sess.run(self.initial_state)\n",
        "      for ii, batch_x in enumerate(create_batch_generator(X_data,\n",
        "                                                          None, batch_size=self.batch_size), 1):\n",
        "        feed = {\n",
        "            'tf_x:0': batch_x,\n",
        "            'tf_keepprob:0': 1.0,\n",
        "            self.initial_state : test_state\n",
        "        }\n",
        "        if return_proba:\n",
        "          pred, test_state = sess.run(['probabilities:0', self.final_state], feed_dict=feed)\n",
        "        else :\n",
        "          pred, test_state = sess.run(['labels:0', self.final_state], feed_dict=feed)\n",
        "\n",
        "        preds.append(pred)\n",
        "    return np.concatenate(preds)\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "T9dnsNR_0Qj2"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Implementacion en tensorFlow 2.x\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "class SentimentRNN:\n",
        "    def __init__(self, n_words: int, seq_len: int = 200, lstm_size: int = 256,\n",
        "                 num_layers: int = 1, batch_size: int = 64, learning_rate: float = 0.0001, embedding_size: int = 200):\n",
        "        self.n_words = n_words\n",
        "        self.seq_len = seq_len\n",
        "        self.lstm_size = lstm_size\n",
        "        self.num_layers = num_layers\n",
        "        self.batch_size = batch_size\n",
        "        self.learning_rate = learning_rate\n",
        "        self.embedding_size = embedding_size\n",
        "\n",
        "        # Define the model\n",
        "        self.model = self.build_model()\n",
        "\n",
        "    def build_model(self):\n",
        "        model = tf.keras.Sequential()\n",
        "\n",
        "        # Capa de Embedding\n",
        "        model.add(tf.keras.layers.Embedding(self.n_words, self.embedding_size, input_length=self.seq_len))\n",
        "\n",
        "        # Celdas LSTM\n",
        "        for _ in range(self.num_layers):\n",
        "            model.add(tf.keras.layers.LSTM(self.lstm_size, return_sequences=True, dropout=0.5))\n",
        "\n",
        "        model.add(tf.keras.layers.LSTM(self.lstm_size, dropout=0.5))  # Última LSTM sin return_sequences\n",
        "\n",
        "        # Capa densa\n",
        "        model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "        # Compilamos el modelo\n",
        "        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=self.learning_rate),\n",
        "                      loss='binary_crossentropy',\n",
        "                      metrics=['accuracy'])\n",
        "\n",
        "        return model\n",
        "\n",
        "    def train(self, X_train, y_train, num_epochs):\n",
        "        # Convertir los datos a TensorFlow Dataset para un procesamiento eficiente\n",
        "        dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
        "        dataset = dataset.batch(self.batch_size).shuffle(1000)\n",
        "\n",
        "        # Entrenamos el modelo\n",
        "        self.model.fit(dataset, epochs=num_epochs)\n",
        "\n",
        "    def predict(self, X_data, return_proba=False):\n",
        "        # Realizamos la predicción\n",
        "        predictions = self.model.predict(X_data, batch_size=self.batch_size)\n",
        "\n",
        "        if return_proba:\n",
        "            return predictions\n",
        "        else:\n",
        "            return (predictions > 0.5).astype(int)\n",
        "\n",
        "# Ejemplo de uso:\n",
        "# n_words = vocabulario tamaño\n",
        "# seq_len = longitud de secuencias de entrada\n",
        "# sentiment_rnn = SentimentRNN(n_words=10000, seq_len=200)\n",
        "# sentiment_rnn.train(X_train, y_train, num_epochs=10)\n",
        "# predicciones = sentiment_rnn.predict(X_test)"
      ],
      "metadata": {
        "id": "OT04kzOZtR0R"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "source": [
        "# In the cell where X_train and y_train are defined\n",
        "X_train = sequences[:25000, :]\n",
        "y_train = df.loc[:24999, 'sentiment'].values  # Changed to 24999 to match X_train shape\n",
        "X_test = sequences[25000:, :]\n",
        "y_test = df.loc[25000:, 'sentiment'].values"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "qZSM1Ki6xX-I"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_words = max(list(word_to_int.values())) + 1\n",
        "\n",
        "rnn = SentimentRNN(n_words=n_words, seq_len=sequence_length,\n",
        "                   embedding_size=256, lstm_size=128,\n",
        "                   num_layers=1, batch_size=64,\n",
        "                   learning_rate=0.0001)\n",
        "rnn.train(X_train, y_train, num_epochs=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qXGj7q44rcXl",
        "outputId": "185708b2-8dd8-4732-a9e6-8fb812f35e69"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m733s\u001b[0m 2s/step - accuracy: 0.5719 - loss: 0.6688\n",
            "Epoch 2/10\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m720s\u001b[0m 2s/step - accuracy: 0.8189 - loss: 0.4007\n",
            "Epoch 3/10\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m744s\u001b[0m 2s/step - accuracy: 0.8839 - loss: 0.2914\n",
            "Epoch 4/10\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m739s\u001b[0m 2s/step - accuracy: 0.9067 - loss: 0.2320\n",
            "Epoch 5/10\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m745s\u001b[0m 2s/step - accuracy: 0.9231 - loss: 0.1947\n",
            "Epoch 6/10\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m758s\u001b[0m 2s/step - accuracy: 0.9414 - loss: 0.1567\n",
            "Epoch 7/10\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m727s\u001b[0m 2s/step - accuracy: 0.9516 - loss: 0.1323\n",
            "Epoch 8/10\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m747s\u001b[0m 2s/step - accuracy: 0.9537 - loss: 0.1225\n",
            "Epoch 9/10\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m729s\u001b[0m 2s/step - accuracy: 0.9622 - loss: 0.1033\n",
            "Epoch 10/10\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m734s\u001b[0m 2s/step - accuracy: 0.9722 - loss: 0.0772\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preds = rnn.predict(X_test)\n",
        "y_true = y_test[:len(preds)]\n",
        "print(f'Precision: {np.sum(preds==y_true)/len(y_test)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bFQZfHYZEBGk",
        "outputId": "237bbaa7-e343-4501-8092-0c0ab777aa8e"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m163s\u001b[0m 417ms/step\n",
            "Precision: 12500.35208\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Si las predicciones son probabilidades, las convertimos a etiquetas binarias\n",
        "# (en caso de que la red esté usando activación sigmoid, lo que devuelve probabilidades)\n",
        "preds = (preds > 0.5).astype(int)\n",
        "\n",
        "# Aseguramos que preds sea un vector unidimensional\n",
        "preds = preds.flatten()\n",
        "\n",
        "# Ahora obtenemos las etiquetas verdaderas correspondientes a X_test\n",
        "y_true = y_test[:len(preds)]\n",
        "\n",
        "# Calculamos la precisión\n",
        "precision = np.sum(preds == y_true) / len(y_true)\n",
        "print(f'Precisión: {precision}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Fx_MlR_SacO",
        "outputId": "d5ba8e02-1b94-4f0d-d616-7c41aaff592a"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precisión: 0.85\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rnn.model.save('rnn_mivie_data_model.h5')\n",
        "print('modelo guardado')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AQ9Be5lCTGxP",
        "outputId": "f6e0042c-f800-4532-866f-f7f3e42b931c"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "modelo guardado\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Proyecto dos: Implementar una RNN para el modelado del lenguaje a nivel de carácter en TensorFlow."
      ],
      "metadata": {
        "id": "a9kRtjfGJs0j"
      }
    }
  ]
}